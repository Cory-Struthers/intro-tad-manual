---
title: "Manual Content Analysis"
subtitle: "Introduction to Text as Data"
author: "Amber Boydstun & Cory Struthers"
date: "April 27-29, 2023"
output:
  html_document:
    toc: yes
    df_print: paged
  html_notebook:
    code_folding: show
    highlight: tango
    theme: united
    toc: yes
    df_print: paged
---



```{r, setup, include=FALSE}
knitr::opts_knit$set(root.dir = "~/Dropbox/text-as-data-JUST-CORY-AND-AMBER/modules/data/")
```

### Introduction

The goal of this module is to get (re)familiar with R, think about our measurement goals, walk through the joys and challenges of manual content analysis, and appreciate the necessity of humans reading of text.

We will need the following packages. We also need to set our working directory to use relative paths when loading data. Time constraints prevent us from diving deeply into differences between absolute and relative paths, but this [source](https://www.r4epi.com/file-paths.html) provides helpful background. Relative paths are considered best practice for collaborative projects, so that is what we'll set up here using `setwd()`. We will use the same line of code every time we begin a new module. Note that on Windows, we use / (forward slash) instead of \ (back slash).

First, set your working directory to the local or remote folder that hosts the data required for this course.

```{r, message=FALSE}

# Load packages
require(tidyverse)
require(dplyr)

# Set working directory
setwd("~/Dropbox/text-as-data-JUST-CORY-AND-AMBER/modules/data/")
#getwd() # view working directory

```

\

### Stop and think about your measurement goals!

Before you do anything, with any text, Step 1 is to think hard about your measurement goals, namely your research question and exactly what you want to measure. Your measurement goals should guide all other decisions. This is true for manual content analysis but also for text-as-data approaches of all kinds.

#### What is your research question?

Do you want to explore some text, without strong expectations of what it holds? Do you want to test a pre-formulated hypothesis about specific patterns you expect to find in the text? Do you want to use the text to measure a key variable that you will then use alongside other metrics for a bigger project? Whatever your motivations are for picking up a body of text and analyzing it, you want to have those motivations solid---preferably in writing---before you begin. The land of text as data is a magical land, but one filled with many rabbit holes where one can easily get lost.

#### What is your latent variable(s) of interest?

Unless your goal is to explore a body of text without any expectations of what you will find, you should think conceptually about what variable(s) you want to measure. Maybe you're interested in comparing positive and negative language between different news sources, or examining whether the level of emotion expressed in presidential speeches has changed over time, or assessing how similar different high school history texts are. Whatever your research goal, it's important to think long and hard about the core thing (i.e., variable) you want to measure and the pros and cons of different approaches to measuring it.

#### What population of text should you use?

For example, if you're interested in the discussion of pets by young adults on social media in the last year, you probably don't want to look at Facebook, or even Twitter. You probably want to look at Instagram, Snapchat, and/or TikTok, and you might want to subset your population to only those posts authored by people under a certain age. However, if you're interested in comparing how different people of different ages talk about their pets on social media, you'd likely want to include all users across all these platforms (and more).

#### What unit of analysis should you use?

An often overlooked but *crucial* question is what unit of analysis you should use. In many cases, the unit of analysis is the **document**. For instance, some researchers may want to measure how many times each newspaper article or social media post references a term like "liberal", "conservative", or (better yet) "giraffe". However, in some cases the unit of analysis might be smaller or larger than the document: the analyst may want to break apart a newspaper article into paragraphs, or combine all the tweets written by a single author in one day. In our example about discussion of pets, for example, should you use treat each social media post as a unit (i.e., treat it as a separate document)? Or would it be better to break up the posts into sentences, with the sentence as the unit of analysis? Or should you expand it out, pooling all the posts by month, and treating the month as your unit of analysis, with all posts on that month treated as one big document? There are no wrong answers here. Which unit of analysis you should use depends on---you guessed it---your research question/goal and your latent variable(s) of interest.

\

<center>![](/Users/cs86487/Dropbox/text-as-data-JUST-CORY-AND-AMBER/images/conservative giraffe.jpeg){width="70%"}</center>
<center>Credit: Michelle Budge</center>

\

### Look at your data

Once you've made these important decisions, the next step for manual content analysis is to LOOK at your data, keeping your research goal and latent variable(s) of interest in mind. Let's use a sample of Twitter posts about immigration as an example. For this exercise, let's use the tweet as our unit of analysis.

``` {r, message = FALSE}

# Load data
tweet_data = read.csv("sample_immigration_tweets_2013-2017.csv")
class(tweet_data)

# Look at your text data
head(tweet_data$text, 15)

```

Think about a key variable you'd like to measure, and scan through the tweets to get a sense for how easy it would be to categorize each tweet according to your variable. For a real project, we'd want to take a random sample of the text and read it at this stage.

### Build and test a codebook

With your variable in mind, you'll then want to write down the rules that will allow you (and future researchers) to categorize each text according to that variable. For example, if we want to track social media posts about pets, we'll want to establish a rule telling us whether or not someone's post about a *friend's* pet should count.

### Code your data independent from another coder

As soon as you have your codebook ready, work independently from your teammate(s) to code your data.

### Check for inter-coder reliability

Use Deen Freelon's extraordinary (and free!) online system to calculate pairwise inter-coder reliability:
http://dfreelon.org/utils/recalfront/

In general, a Cohen's Kappa and Krippendorff's Alpha scores of over 0.7 allow us to be confident in the reliability of the coding.

### Wash, rinse, and repeat

You'll want to iterate through Steps 1-5 until you have a solid concept of your variable of interest, a strong codebook that can handle most new observations you code, and high levels of inter-coder reliability. Make sure to annotate your codebook as you go, making notes of any specific coding decisions you make. But careful! If you make a decision that is inconsistent with how you have coded things in the past, you'll need to go back and find those observations and re-code them according to your new rule. Fun, right?


\

---

**Question 1 (BREAKOUT). Work with your partner or team to develop a variable of interest and a brief codebook. What was the process like? What were the easy parts, and what were the challenges?** 

---


**Question 2 (BREAKOUT). Using the code below, subset the immigration twitter data to the same 20 observations, and each code those 20 tweets independently. What was the process like? How confident did you feel in your coding, and what did you observe from spending time with the text?**

``` {r, message = FALSE}

# Subset to only 20 observations and only the text variable
tweet_text_df = tweet_data  %>%
  select(text) %>%
  slice(100:119)

# Add two columns for your own codings, enter codings within quotation marks like the example
valence = c("")
other_variable = c("")

tweet_data_coded = cbind(tweet_text_df, valence, other_variable)

# Output your data for manual coding (writing into your current working directory folder)
write.csv(tweet_data_coded, "tweet_data_coded.csv", row.names=TRUE)

```

---

**Question 3 (BREAKOUT). Calculate your inter-coder reliability (follow Professor Freelon's instructions about how to format the .csv file!). What factors help explain how high or low it is? Which particular observations did you code differently, and how might you update the coding process to improve the inter-coder reliability?**

---

**Question 4 (BREAKOUT). Finally, discuss: what worked and what didn't, and why?**

\


